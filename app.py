import streamlit as st
import cv2
import numpy as np
import io
import wave
import plotly.graph_objects as go

st.set_page_config(layout="wide", page_title="120BPM Optical DAW")
st.title("ğŸ§ 120 BPM Sync: Professional Optical DAW")

# --- ìŒì•…ì  ì„¤ì • ---
BPM = 120
BEAT_DURATION = 60 / BPM  # 1ë°•ì ê¸¸ì´ (0.5ì´ˆ)
SUBDIVISION = 4           # 1ë°•ìë¥¼ 4ê°œë¡œ ìª¼ê°œê¸° (16ë¶„ìŒí‘œ ë‹¨ìœ„ ë¶„ì„)
SAMPLE_RATE = 22050

def apply_eq_and_envelope(tone, layer_idx):
    """ë ˆì´ì–´ë³„ íŠ¹í™” EQ ë° ì—”ë²¨ë¡œí”„ ì ìš©"""
    n = len(tone)
    t_env = np.linspace(0, 1, n)
    
    if layer_idx == 0:  # ğŸ¸ Deep Bass: ì €ìŒì—­ëŒ€ ê°•ì¡°, ê³ ìŒ ì»¤íŠ¸ (Low Pass)
        env = np.ones(n)
        # ëë¶€ë¶„ë§Œ ì‚´ì§ í˜ì´ë“œì•„ì›ƒí•˜ì—¬ ì›…ì¥í•¨ ìœ ì§€
        env[-int(n*0.2):] = np.linspace(1, 0, int(n*0.2))
        return tone * env * 0.9
    
    elif layer_idx == 1:  # ğŸ¹ Warm Pluck: ì¤‘ìŒì—­ëŒ€ ê°•ì¡°, ì§§ì€ íƒ€ê²©ê°
        env = np.exp(-t_env * 10)  # ì•„ì£¼ ë¹ ë¥´ê²Œ ì‚¬ë¼ì§€ëŠ” ì†Œë¦¬
        return tone * env * 0.7
    
    elif layer_idx == 2:  # ğŸ¤ Airy Lead: ì¤‘ê³ ìŒì—­ëŒ€, ë¶€ë“œëŸ¬ìš´ ì—°ê²°
        env = np.sin(t_env * np.pi) # ë¶€ë“œëŸ½ê²Œ ì‹œì‘í•´ì„œ ë¶€ë“œëŸ½ê²Œ ëë‚¨
        return tone * env * 0.5
    
    else:  # âœ¨ Shimmer Bell: ê³ ìŒì—­ëŒ€ ì „ìš©, ì”í–¥ ê°•ì¡°
        env = np.exp(-t_env * 5)
        return tone * env * 0.4

def generate_musical_wave(freq, duration, layer_idx):
    """ë ˆì´ì–´ íŠ¹ì„±ì— ë§ëŠ” íŒŒí˜• ìƒì„±"""
    t = np.linspace(0, duration, int(SAMPLE_RATE * duration), False)
    
    if layer_idx == 0: # Bass: Sine + 1ì˜¥íƒ€ë¸Œ ìœ„ ë°°ìŒ ì‚´ì§
        wave = np.sin(2 * np.pi * freq * t) + 0.2 * np.sin(2 * np.pi * freq * 2 * t)
    elif layer_idx == 1: # Pluck: Square(Filtered ëŠë‚Œ) + Sine
        wave = 0.5 * np.sign(np.sin(2 * np.pi * freq * t)) + 0.5 * np.sin(2 * np.pi * freq * t)
    elif layer_idx == 2: # Lead: Sawtooth(ë¶€ë“œëŸ½ê²Œ)
        wave = 2 * (t * freq - np.floor(0.5 + t * freq))
    else: # Bell: Sine FM í•©ì„± ëŠë‚Œ
        wave = np.sin(2 * np.pi * freq * t + 0.5 * np.sin(2 * np.pi * freq * 2.01 * t))
        
    return apply_eq_and_envelope(wave, layer_idx)

with st.sidebar:
    st.header("ğŸ› Global Settings")
    uploaded_file = st.file_uploader("ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['mp4', 'mov', 'avi'])
    st.divider()
    sensitivity = st.slider("ê´‘ì› ê°ë„", 50, 255, 180)
    master_gain = st.slider("Master Output", 0.5, 5.0, 1.8)
    st.info(f"í˜„ì¬ í…œí¬: {BPM} BPM (16ë¶„ìŒí‘œ ë‹¨ìœ„ ë¶„ì„)")

if uploaded_file:
    try:
        with open("temp_video.mp4", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        cap = cv2.VideoCapture("temp_video.mp4")
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        video_duration = total_frames / fps
        
        # ë¶„ì„ ë‹¨ìœ„ ì„¤ì • (16ë¶„ìŒí‘œ ê¸¸ì´ë§Œí¼ í”„ë ˆì„ì„ ë¬¶ì–´ì„œ ë¶„ì„)
        unit_duration = BEAT_DURATION / SUBDIVISION
        frames_per_unit = int(fps * unit_duration)
        num_units = int(video_duration / unit_duration)
        
        tracks_l = [np.zeros(int(SAMPLE_RATE * video_duration) + SAMPLE_RATE) for _ in range(4)]
        tracks_r = [np.zeros(int(SAMPLE_RATE * video_duration) + SAMPLE_RATE) for _ in range(4)]
        vis_pitches = [[] for _ in range(4)]
        
        prog = st.progress(0)
        
        for u in range(num_units):
            # í•´ë‹¹ ë°•ì ë‹¨ìœ„ì˜ ì¤‘ê°„ í”„ë ˆì„ ì¶”ì¶œ (ë¶„ì ˆ ê°ì†Œë¥¼ ìœ„í•´ ëŒ€í‘œê°’ ì‚¬ìš©)
            frame_idx = u * frames_per_unit
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if not ret: break
            
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, sensitivity, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            sorted_cnts = sorted(contours, key=cv2.contourArea, reverse=True)[:4]
            start_sample = int(u * unit_duration * SAMPLE_RATE)
            
            for idx, cnt in enumerate(sorted_cnts):
                area = cv2.contourArea(cnt)
                M = cv2.moments(cnt)
                if M["m00"] == 0: continue
                cx = int(M["m10"]/M["m00"])
                
                # ì£¼íŒŒìˆ˜ë¥¼ ìŒì•…ì  ìŠ¤ì¼€ì¼ì— ë§ì¶° ì¡°ì • (C major scale ëŠë‚Œ)
                scale = [65.41, 130.81, 261.63, 523.25] # C2, C3, C4, C5
                freq = scale[idx] + (area % 50)
                
                # ìŒì•…ì  ë‹¨ìœ„(16ë¶„ìŒí‘œ) ê¸¸ì´ì˜ ì†Œë¦¬ ìƒì„±
                tone = generate_musical_wave(freq, unit_duration, idx)
                
                pan_r = np.clip(cx / frame.shape[1], 0.1, 0.9)
                pan_l = 1.0 - pan_r
                
                end_sample = start_sample + len(tone)
                if end_sample < len(tracks_l[0]):
                    tracks_l[idx][start_sample:end_sample] += tone * pan_l * master_gain
                    tracks_r[idx][start_sample:end_sample] += tone * pan_r * master_gain
                vis_pitches[idx].append(freq)

            for j in range(len(sorted_cnts), 4): vis_pitches[j].append(None)
            if u % 10 == 0: prog.progress(min(u / num_units, 1.0))

        # ë¯¹ì‹± ë° ë…¸ë©€ë¼ì´ì§•
        final_l = np.sum(tracks_l, axis=0)
        final_r = np.sum(tracks_r, axis=0)
        master_stereo = np.vstack((final_l, final_r)).T
        peak = np.max(np.abs(master_stereo))
        if peak > 0: master_stereo = (master_stereo / peak) * 0.85
        audio_int16 = (master_stereo * 32767).astype(np.int16)

        wav_buf = io.BytesIO()
        with wave.open(wav_buf, 'wb') as wf:
            wf.setnchannels(2); wf.setsampwidth(2); wf.setframerate(SAMPLE_RATE); wf.writeframes(audio_int16.tobytes())

        # --- UI ë ˆì´ì•„ì›ƒ ---
        st.header("ğŸµ 120 BPM Quantized Mix")
        col_v, col_g = st.columns([1, 1])
        
        with col_v:
            st.video(uploaded_file)
            st.audio(wav_buf.getvalue())
            st.download_button("ğŸ’¾ ì „ì²´ ë¯¹ìŠ¤ ì €ì¥", wav_buf.getvalue(), "quantized_mix.wav")

        with col_g:
            time_axis = np.linspace(0, video_duration, len(vis_pitches[0]))
            fig = go.Figure()
            colors = ['#00E5FF', '#FF3D00', '#D500F9', '#FFEA00']
            for i in range(4):
                fig.add_trace(go.Scatter(x=time_axis, y=vis_pitches[i], name=f"Layer {i+1}", line=dict(color=colors[i], width=2)))
            fig.update_layout(template="plotly_dark", height=420, xaxis=dict(rangeslider=dict(visible=True)))
            st.plotly_chart(fig, use_container_width=True)

        st.divider()
        st.subheader("ğŸ“ ê°œë³„ ë ˆì´ì–´ ìŠ¤í…Œë ˆì˜¤ íŒŒì¼ (EQ ì ìš©ë¨)")
        cols = st.columns(4)
        layer_names = ["Deep Bass", "Warm Pluck", "Airy Lead", "Shimmer Bell"]
        for i in range(4):
            with cols[i]:
                t_buf = io.BytesIO()
                t_data = np.vstack((tracks_l[i], tracks_r[i])).T
                t_peak = np.max(np.abs(t_data))
                if t_peak > 0: t_data = (t_data / t_peak) * 0.8
                with wave.open(t_buf, 'wb') as wf:
                    wf.setnchannels(2); wf.setsampwidth(2); wf.setframerate(SAMPLE_RATE); wf.writeframes((t_data * 32767).astype(np.int16).tobytes())
                st.write(f"**{layer_names[i]}**")
                st.audio(t_buf.getvalue())
                st.download_button(f"ğŸ“¥ {i+1}ë²ˆ ì €ì¥", t_buf.getvalue(), f"track_{i+1}.wav")

    except Exception as e:
        st.error(f"ì˜¤ë¥˜: {e}")
