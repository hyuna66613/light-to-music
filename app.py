import streamlit as st
import cv2
import numpy as np
import io
import wave
import plotly.graph_objects as go

st.set_page_config(layout="wide", page_title="Electro Light DAW")
st.title("ğŸš€ Electro Light: Synth & Beat DAW")

# --- ì¼ë ‰íŠ¸ë¡œë‹‰ ì‚¬ìš´ë“œ ë””ìì¸ (MIDI ë ˆí¼ëŸ°ìŠ¤) ---
# 4ê°œì˜ íŠ¸ë™ì— ê°ê¸° ë‹¤ë¥¸ ì•…ê¸° ì„±ê²©ì„ ë¶€ì—¬ (ë² ì´ìŠ¤, ë¦¬ë“œ, ì‹ ìŠ¤, í¼ì»¤ì…˜ ëŠë‚Œ)
SCALES = [
    [55.00, 65.41, 73.42, 82.41], # Track 1: Deep Bass (E-G-A-B)
    [110.00, 130.81, 146.83, 164.81], # Track 2: Mid Synth
    [220.00, 261.63, 293.66, 329.63], # Track 3: High Lead
    [440.00, 523.25, 587.33, 659.25]  # Track 4: Shimmer/Perc
]

def apply_electronic_synth(tone, sample_rate, brightness):
    n = len(tone)
    # 1. ì¼ë ‰íŠ¸ë¡œë‹‰ íŠ¹ìœ ì˜ ë‚ ì¹´ë¡œìš´ íŒŒí˜• (ì‚¬ê°íŒŒ í˜¼í•©)
    square = np.sign(tone) * np.abs(tone)
    tone = (tone * 0.7) + (square * 0.3 * (brightness / 255))
    
    # 2. ADSR: ì¼ë ‰íŠ¸ë¡œë‹‰ íŠ¹ìœ ì˜ í†¡ ì˜ëŠ” Attack
    env = np.ones(n, dtype=np.float32)
    attack = int(n * 0.1) # ì§§ê³  ê°•í•œ ì‹œì‘
    release = int(n * 0.6) # ê¸´ ì”í–¥
    env[:attack] = np.linspace(0, 1, attack)
    env[-release:] = np.linspace(1, 0, release)
    
    # 3. Low Pass Filter íš¨ê³¼ (ë°ê¸°ì— ë”°ë¼ ì†Œë¦¬ì˜ ë¨¹ë¨¹í•¨ ì¡°ì ˆ)
    # ì‹¤ì œ í•„í„° ëŒ€ì‹  ê³ ì£¼íŒŒ ì„±ë¶„ ì œì–´ë¡œ ì‹œë®¬ë ˆì´ì…˜
    return tone * env

with st.sidebar:
    st.header("ğŸ› Synth Engine")
    uploaded_file = st.file_uploader("ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['mp4', 'mov', 'avi'])
    st.divider()
    sensitivity = st.slider("ë¹› ê°ì§€ ë¯¼ê°ë„ (ë‚®ì„ìˆ˜ë¡ ì˜ ì¡í˜)", 100, 250, 180)
    master_gain = st.slider("ë§ˆìŠ¤í„° ë³¼ë¥¨", 0.1, 2.0, 1.2)

if uploaded_file:
    try:
        with open("temp_video.mp4", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        cap = cv2.VideoCapture("temp_video.mp4")
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_rate = 22050 
        
        # 4ê°œ ë ˆì´ì–´ ì„¤ì •
        num_tracks = 4
        tracks_l = [np.zeros(int(sample_rate * (total_frames / fps)) + sample_rate) for _ in range(num_tracks)]
        tracks_r = [np.zeros(int(sample_rate * (total_frames / fps)) + sample_rate) for _ in range(num_tracks)]
        visual_data = [[] for _ in range(num_tracks)]
        
        prog = st.progress(0)
        for i in range(total_frames):
            ret, frame = cap.read()
            if not ret: break
            
            # ì¼ë°˜í™”ë¥¼ ìœ„í•´ ë°ê¸°/í¬ê¸°/ë©´ì  ì¶”ì¶œ
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            # ë°ê¸° ê°•ì¡°ë¥¼ ìœ„í•´ ë¯¼ê°ë„ ì ìš©
            _, thresh = cv2.threshold(gray, sensitivity, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            start_idx = int(i * (sample_rate / fps))
            t = np.linspace(0, 1/fps, int(sample_rate/fps), False).astype(np.float32)
            
            # ìƒìœ„ 4ê°œ ê´‘ì› ë¶„ì„
            sorted_cnts = sorted(contours, key=cv2.contourArea, reverse=True)[:num_tracks]
            
            for idx, cnt in enumerate(sorted_cnts):
                area = cv2.contourArea(cnt)
                # í‰ê·  ë°ê¸° ê³„ì‚°
                mask = np.zeros(gray.shape, np.uint8)
                cv2.drawContours(mask, [cnt], -1, 255, -1)
                mean_brightness = cv2.mean(gray, mask=mask)[0]
                
                M = cv2.moments(cnt)
                if M["m00"] == 0: continue
                cx = int(M["m10"]/M["m00"])
                
                # [í•µì‹¬] ë©´ì ì— ë”°ë¼ ìŒë†’ì´ ê²°ì •, ë°ê¸°ì— ë”°ë¼ ìŒìƒ‰ ê²°ì •
                scale = SCALES[idx]
                note_idx = int((area % 1000) / 250) % len(scale)
                freq = scale[note_idx]
                
                # ë°ê¸°ì™€ ë©´ì ì„ ì¡°í•©í•œ ë³¼ë¥¨ (ì‘ì€ ë¶ˆë¹›ë„ ë°ìœ¼ë©´ ì†Œë¦¬ê°€ ë‚˜ê²Œ ì„¤ì •)
                vol = (min(area / 1000, 0.6) * 0.5 + (mean_brightness / 255) * 0.5) * master_gain
                
                # ë¯¸ë”” ìŠ¤íƒ€ì¼ í•©ì„±ìŒ ìƒì„±
                tone = vol * np.sin(2 * np.pi * freq * t)
                tone = apply_electronic_synth(tone, sample_rate, mean_brightness)
                
                # ìŠ¤í…Œë ˆì˜¤ íŒ¬ë‹
                pan_r = cx / frame.shape[1]
                pan_l = 1.0 - pan_r
                
                end_idx = start_idx + len(tone)
                if end_idx < len(tracks_l[0]):
                    tracks_l[idx][start_idx:end_idx] += tone * pan_l
                    tracks_r[idx][start_idx:end_idx] += tone * pan_r
                
                visual_data[idx].append(freq)
            
            # ë°ì´í„° ë™ê¸°í™”
            for j in range(len(sorted_cnts), num_tracks):
                visual_data[j].append(None)
            
            if i % 30 == 0: prog.progress(i / total_frames)

        # ë¯¹ì‹±
        master_l = np.sum(tracks_l, axis=0)
        master_r = np.sum(tracks_r, axis=0)
        master_stereo = np.vstack((master_l, master_r)).T
        
        # ì•ˆì „í•œ ë…¸ë©€ë¼ì´ì§•
        peak = np.max(np.abs(master_stereo))
        if peak > 0: master_stereo = (master_stereo / peak) * 0.8
        audio_int16 = np.clip(master_stereo * 32767, -32768, 32767).astype(np.int16)

        # WAV ë°”ì´ë„ˆë¦¬
        wav_buf = io.BytesIO()
        with wave.open(wav_buf, 'wb') as wf:
            wf.setnchannels(2); wf.setsampwidth(2); wf.setframerate(sample_rate)
            wf.writeframes(audio_int16.tobytes())

        # --- UI ---
        col1, col2 = st.columns([1, 1])
        with col1:
            st.header("ğŸ Video & MIDI Player")
            st.video(uploaded_file)
            st.audio(wav_buf.getvalue())
            
            # ë ˆì´ì–´ë³„ ì €ì¥ ê¸°ëŠ¥ ë³µêµ¬
            st.subheader("Layer Export")
            selected_layer = st.selectbox("ì €ì¥í•  ë ˆì´ì–´ ì„ íƒ", [f"Track {i+1}" for i in range(num_tracks)])
            layer_idx = int(selected_layer.split()[-1]) - 1
            
            # ê°œë³„ ë ˆì´ì–´ WAV ìƒì„±
            l_buf = io.BytesIO()
            l_data = np.vstack((tracks_l[layer_idx], tracks_r[layer_idx])).T
            l_peak = np.max(np.abs(l_data))
            if l_peak > 0: l_data = (l_data / l_peak) * 0.8
            l_int16 = np.clip(l_data * 32767, -32768, 32767).astype(np.int16)
            with wave.open(l_buf, 'wb') as wf:
                wf.setnchannels(2); wf.setsampwidth(2); wf.setframerate(sample_rate)
                wf.writeframes(l_int16.tobytes())
            
            st.download_button(f"ğŸ’¾ {selected_layer} ë‹¤ìš´ë¡œë“œ", l_buf.getvalue(), f"{selected_layer}.wav")

        with col2:
            st.header("ğŸ“Š MIDI Timeline")
            time_axis = np.linspace(0, total_frames/fps, total_frames)
            fig = go.Figure()
            colors = ['#FF4B4B', '#1C83E1', '#00D1FF', '#7752FE']
            for i in range(num_tracks):
                fig.add_trace(go.Scatter(x=time_axis, y=visual_data[i], name=f"Track {i+1}", line=dict(color=colors[i], width=2)))
            
            fig.update_layout(template="plotly_dark", height=450, margin=dict(l=10, r=10, t=10, b=10),
                            xaxis_title="Time (sec)", yaxis_title="Note Pitch", hovermode="x unified")
            st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"ì—°ì‚° ì˜¤ë¥˜: {e}")
