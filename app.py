import streamlit as st
import cv2
import numpy as np
from scipy.io import wavfile
import io
import plotly.graph_objects as go

st.set_page_config(layout="wide", page_title="Musical Light DAW")
st.title("ğŸ¹ Musical Light: Harmonic Synth DAW")

# --- ìŒì•…ì  ì„¤ì •: ë§ˆì´ë„ˆ íœíƒ€í† ë‹‰ ìŠ¤ì¼€ì¼ (ë°¤ì˜ ëª½í™˜ì ì¸ ëŠë‚Œ) ---
# ë„(C), ë¯¸b(Eb), íŒŒ(F), ì†”(G), ì‹œb(Bb) ì£¼íŒŒìˆ˜ ë¦¬ìŠ¤íŠ¸
NOTES = [130.81, 155.56, 174.61, 196.00, 233.08, 
         261.63, 311.13, 349.23, 392.00, 466.16, 
         523.25, 622.25, 698.46, 783.99, 932.33]

def get_nearest_note(freq):
    return min(NOTES, key=lambda x: abs(x - freq))

def apply_envelope(tone, sample_rate):
    # ë¶€ë“œëŸ¬ìš´ ì‹œì‘(Attack)ê³¼ ë(Release) ì²˜ë¦¬
    n = len(tone)
    attack = int(sample_rate * 0.01)
    release = int(sample_rate * 0.05)
    env = np.ones(n)
    env[:attack] = np.linspace(0, 1, attack)
    env[-release:] = np.linspace(1, 0, release)
    return tone * env

with st.sidebar:
    st.header("ğŸ› Synth Engine")
    uploaded_file = st.file_uploader("ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['mp4', 'mov', 'avi'])
    harmony_mode = st.select_slider("Harmony Style", options=["Deep", "Dreamy", "Sharp"])

if uploaded_file:
    try:
        with open("temp_video.mp4", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        cap = cv2.VideoCapture("temp_video.mp4")
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        sample_rate = 22050 
        max_tracks = 6
        tracks_audio_l = [[] for _ in range(max_tracks)] # ì™¼ìª½ ì±„ë„
        tracks_audio_r = [[] for _ in range(max_tracks)] # ì˜¤ë¥¸ìª½ ì±„ë„
        tracks_visual = [[] for _ in range(max_tracks)]
        
        prog = st.progress(0)
        for i in range(total_frames):
            ret, frame = cap.read()
            if not ret: break
            
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            duration = 1.0 / fps
            t = np.linspace(0, duration, int(sample_rate * duration), False)
            sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:max_tracks]
            
            for idx, cnt in enumerate(sorted_contours):
                area = cv2.contourArea(cnt)
                M = cv2.moments(cnt)
                if M["m00"] == 0: continue
                cx, cy = int(M["m10"] / M["m00"]), int(M["m01"] / M["m00"])
                
                # 1. ì£¼íŒŒìˆ˜ë¥¼ ìŒê³„(Scale)ì— ë§ì¶¤
                raw_freq = 150 + ((frame.shape[0] - cy) * 1.5)
                note_freq = get_nearest_note(raw_freq)
                
                # 2. ë°°ìŒ ì¶”ê°€ (ì§„ì§œ ì•…ê¸°ì²˜ëŸ¼ ë“¤ë¦¬ê²Œ í•¨)
                vol = min(area / 1500, 0.6)
                tone = vol * np.sin(2 * np.pi * note_freq * t) # ê¸°ë³¸ìŒ
                tone += (vol * 0.3) * np.sin(2 * np.pi * (note_freq * 2) * t) # ì˜¥íƒ€ë¸Œ ë°°ìŒ
                
                # 3. ë¶€ë“œëŸ¬ìš´ ADSR ì ìš©
                tone = apply_envelope(tone, sample_rate)
                
                # 4. íŒ¬ë‹(Panning): xì¢Œí‘œì— ë”°ë¥¸ ì…ì²´ ìŒí–¥
                pan_r = cx / frame.shape[1]
                pan_l = 1 - pan_r
                
                tracks_audio_l[idx].append(tone * pan_l)
                tracks_audio_r[idx].append(tone * pan_r)
                tracks_visual[idx].append({'time': i/fps, 'freq': note_freq})
            
            for j in range(len(sorted_contours), max_tracks):
                tracks_audio_l[j].append(np.zeros_like(t))
                tracks_audio_r[j].append(np.zeros_like(t))
                tracks_visual[j].append({'time': i/fps, 'freq': 0})
            
            if i % 30 == 0: prog.progress(i / total_frames)

        # ë¯¹ì‹± (ìŠ¤í…Œë ˆì˜¤)
        master_l = np.sum([np.concatenate(t) for t in tracks_audio_l], axis=0)
        master_r = np.sum([np.concatenate(t) for t in tracks_audio_r], axis=0)
        
        # ë…¸ë©€ë¼ì´ì§• ë° ìŠ¤í…Œë ˆì˜¤ í•©ì¹˜ê¸°
        master_stereo = np.vstack((master_l, master_r)).T
        master_stereo = (master_stereo / np.max(np.abs(master_stereo)) * 32767).astype(np.int16)

        # UI ì¶œë ¥ (ìƒëµëœ ë¶€ë¶„ì€ ì´ì „ê³¼ ë™ì¼)
        col1, col2 = st.columns([1, 1])
        with col1:
            st.video(uploaded_file)
            st.audio(master_stereo, sample_rate=sample_rate)
        
        with col2:
            st.header("ğŸ“Š Harmonic Timeline")
            # ì£¼íŒŒìˆ˜ ê·¸ë˜í”„ ì‹œê°í™” (ì½”ë“œ ë™ì¼)
            # ... (ì´ì „ ì‹œê°í™” ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ì ìš©)

    except Exception as e:
        st.error(f"ì˜¤ë¥˜: {e}")
