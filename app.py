import streamlit as st
import cv2
import numpy as np
from scipy.io import wavfile
import io
import plotly.graph_objects as go

# --- ê¸°ë³¸ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="GarageLight DAW")
st.title("ğŸ¼ GarageLight: Optical Synth DAW (Final Fixed)")

# ë°¤ì˜ ë¶„ìœ„ê¸°ì— ì–´ìš¸ë¦¬ëŠ” íœíƒ€í† ë‹‰ ìŒê³„ (ë„, ë ˆ, ë¯¸, ì†”, ë¼ ê¸°ë°˜)
NOTES = [130.81, 146.83, 164.81, 196.00, 220.00, 
         261.63, 293.66, 329.63, 392.00, 440.00, 
         523.25, 587.33, 659.25, 783.99, 880.00]

def get_nearest_note(freq):
    return NOTES[np.abs(np.array(NOTES) - freq).argmin()]

def apply_envelope(tone, sample_rate):
    n = len(tone)
    if n < 100: return tone
    attack = int(min(sample_rate * 0.02, n * 0.15))
    release = int(min(sample_rate * 0.08, n * 0.3))
    env = np.ones(n, dtype=np.float32)
    env[:attack] = np.linspace(0, 1, attack)
    env[-release:] = np.linspace(1, 0, release)
    return tone * env

with st.sidebar:
    st.header("ğŸ› Control Panel")
    uploaded_file = st.file_uploader("ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì†Œë¦¬ëŠ” ë¬´ì‹œë¨)", type=['mp4', 'mov', 'avi'])
    st.divider()
    vol_boost = st.slider("ë§ˆìŠ¤í„° ë³¼ë¥¨ ì¦í­", 0.5, 3.0, 1.0)

if uploaded_file:
    try:
        # ì˜ìƒ ì„ì‹œ ì €ì¥
        with open("temp_video.mp4", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        cap = cv2.VideoCapture("temp_video.mp4")
        fps = cap.get(cv2.CAP_PROP_FPS) or 30
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_rate = 22050 
        
        # ì˜¤ë””ì˜¤ ë„í™”ì§€ ìƒì„± (float32ë¡œ ì •ë°€ ì—°ì‚°)
        audio_len = int(sample_rate * (total_frames / fps)) + sample_rate
        master_l = np.zeros(audio_len, dtype=np.float32)
        master_r = np.zeros(audio_len, dtype=np.float32)
        
        # ì‹œê°í™” ë°ì´í„° ë³´ê´€í•¨ (ìµœëŒ€ 6íŠ¸ë™)
        tracks_visual = [[] for _ in range(6)]
        
        prog_bar = st.progress(0)
        status = st.empty()

        for i in range(total_frames):
            ret, frame = cap.read()
            if not ret: break
            
            # ë¹› ê°ì§€ (ì˜ìƒ ì†Œë¦¬ëŠ” ì½ì§€ ì•ŠìŒ)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, 235, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            start_idx = int(i * (sample_rate / fps))
            t = np.linspace(0, 1/fps, int(sample_rate/fps), False).astype(np.float32)
            
            # ê°€ì¥ ë°ì€ ë¶ˆë¹› 6ê°œ ì¶”ì¶œ
            sorted_cnts = sorted(contours, key=cv2.contourArea, reverse=True)[:6]
            for idx, cnt in enumerate(sorted_cnts):
                M = cv2.moments(cnt)
                if M["m00"] == 0: continue
                cx, cy = int(M["m10"]/M["m00"]), int(M["m01"]/M["m00"])
                
                # ìŒë†’ì´ì™€ ë³¼ë¥¨ ê³„ì‚°
                note_f = get_nearest_note(150 + (frame.shape[0]-cy)*1.8)
                area_vol = min(cv2.contourArea(cnt)/1200, 0.4) * vol_boost
                
                # ë°°ìŒì´ ì„ì¸ ì „ììŒ ìƒì„±
                tone = area_vol * (np.sin(2 * np.pi * note_f * t) + 0.3 * np.sin(2 * np.pi * note_f * 2 * t))
                tone = apply_envelope(tone, sample_rate)
                
                # ì¢Œìš° ì…ì²´ ìŒí–¥
                pan_r = cx / frame.shape[1]
                pan_l = 1.0 - pan_r
                
                end_idx = start_idx + len(tone)
                if end_idx < audio_len:
                    master_l[start_idx:end_idx] += tone * pan_l
                    master_r[start_idx:end_idx] += tone * pan_r
                
                tracks_visual[idx].append({'t': i/fps, 'f': note_f})
            
            if i % 30 == 0:
                prog_bar.progress(min(i / total_frames, 1.0))
                status.text(f"ğŸšŒ ë°¤ ë²„ìŠ¤ ë¹› ë¶„ì„ ì¤‘... ({i}/{total_frames})")

        status.success("âœ¨ ë¹›ì„ ì†Œë¦¬ë¡œ ëª¨ë‘ ë³€í™˜í–ˆìŠµë‹ˆë‹¤!")

        # --- [ì—ëŸ¬ ë°©ì§€ìš© ì˜¤ë””ì˜¤ ì •ê·œí™”] ---
        master_stereo = np.vstack((master_l, master_r)).T
        
        # 1. ìµœê³  ìŒëŸ‰ ì°¾ê¸°
        peak = np.max(np.abs(master_stereo))
        if peak > 0:
            # 2. ëª¨ë“  ë°ì´í„°ë¥¼ -1.0 ~ 1.0 ë²”ìœ„ë¡œ ì••ì¶• (Normalization)
            master_stereo = master_stereo / peak
            
        # 3. [í•µì‹¬] ë¶€í˜¸ ìˆëŠ” 16ë¹„íŠ¸ ì •ìˆ˜ë¡œ ê°•ì œ ë³€í™˜
        # np.clipì„ í†µí•´ -32768 ~ 32767 ë²”ìœ„ë¥¼ ì ˆëŒ€ ë„˜ì§€ ì•Šê²Œ ê¹ì•„ëƒ„
        audio_final = np.clip(master_stereo * 32767, -32768, 32767).astype(np.int16)

        # --- í™”ë©´ ë°°ì¹˜ ---
        col_v, col_g = st.columns([1, 1])
        
        with col_v:
            st.header("ğŸ“½ Video Stream")
            st.video(uploaded_file)
            st.write("ğŸ¹ í•©ì„±ëœ ë§ˆìŠ¤í„° ìŒì›")
            st.audio(audio_final, sample_rate=sample_rate)
            
            buf = io.BytesIO()
            wavfile.write(buf, sample_rate, audio_final)
            st.download_button("ğŸ’¾ ìŒì•… ë‹¤ìš´ë¡œë“œ (WAV)", buf.getvalue(), "night_bus_music.wav")

        with col_g:
            st.header("ğŸ“Š Frequency Timeline")
            for idx in range(6):
                if tracks_visual[idx]:
                    v_t = [v['t'] for v in tracks_visual[idx]]
                    v_f = [v['f'] for v in tracks_visual[idx]]
                    fig = go.Figure(go.Scatter(x=v_t, y=v_f, mode='lines', line=dict(color='#00d1ff', width=1.5)))
                    fig.update_layout(height=110, margin=dict(l=0,r=0,t=10,b=10), xaxis_title="Time (s)", yaxis_title="Hz", paper_bgcolor="rgba(0,0,0,0)", plot_bgcolor="rgba(0,0,0,0)")
                    st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
