import streamlit as st
import cv2
import numpy as np
import io
import wave
import plotly.graph_objects as go

# íŒŒì¼ ì—…ë¡œë“œ ìš©ëŸ‰ ì œí•œ í•´ì œ ì‹œë®¬ë ˆì´ì…˜ ë° ì„¤ì •
st.set_page_config(layout="wide", page_title="Professional Optical DAW")
st.title("ğŸ¹ Optical Music Station (High Stability)")

# --- ê¸€ë¡œë²Œ ì„¤ì • ---
BPM = 120
SAMPLE_RATE = 22050
BEAT_SEC = 60 / BPM 
UNIT_SEC = BEAT_SEC / 2  # 8ë¶„ ìŒí‘œ ë‹¨ìœ„ ë¶„ì„ (ì‚°ë§Œí•¨ ê°ì†Œ)

def apply_pro_eq(tone, layer_idx, brightness_factor):
    """ë ˆì´ì–´ë³„ EQ ë° í•„í„°: ë¹›ì˜ ë°ê¸°ì— ë”°ë¼ ì†Œë¦¬ì˜ ê°œë°©ê° ì¡°ì ˆ"""
    n = len(tone)
    # ì €ìŒ ë ˆì´ì–´ëŠ” ê³ ìŒ ì»¤íŠ¸, ê³ ìŒ ë ˆì´ì–´ëŠ” ì €ìŒ ì»¤íŠ¸
    if layer_idx == 0: # Bass: ë¬µì§í•˜ê²Œ
        env = np.exp(-np.linspace(0, 2, n)) 
        return tone * env * 1.2
    elif layer_idx == 3: # Bell: ë¹›ì´ ë°ì„ìˆ˜ë¡ ë” ë§‘ê²Œ
        env = np.exp(-np.linspace(0, 15, n))
        return tone * env * (0.5 + brightness_factor)
    else:
        env = np.sin(np.linspace(0, np.pi, n))
        return tone * env

@st.cache_data
def analyze_video_characteristics(video_path):
    """ì˜ìƒ ì „ì²´ì˜ ë¬´ë“œë¥¼ ë¶„ì„í•˜ì—¬ ì‚¬ìš´ë“œ í†¤ ê²°ì •"""
    cap = cv2.VideoCapture(video_path)
    ret, frame = cap.read()
    if not ret: return 0.5, 0.5 # ê¸°ë³¸ê°’
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    avg_v = np.mean(hsv[:,:,2]) / 255  # í‰ê·  ë°ê¸°
    avg_h = np.mean(hsv[:,:,0]) / 180  # í‰ê·  ìƒ‰ìƒ (ì˜¨ë„)
    cap.release()
    return avg_v, avg_h

def generate_wave(freq, duration, layer_idx, mood_v):
    """ì˜ìƒ ë¬´ë“œ(mood_v)ê°€ ë°˜ì˜ëœ íŒŒí˜• ìƒì„±"""
    t = np.linspace(0, duration, int(SAMPLE_RATE * duration), False)
    # ë°ì€ ì˜ìƒì¼ìˆ˜ë¡ íŒŒí˜•ì´ ë‚ ì¹´ë¡œì›Œì§ (ë°°ìŒ ì¶”ê°€)
    if layer_idx == 0:
        wave_data = np.sin(2 * np.pi * freq * t)
    elif layer_idx == 1:
        wave_data = 0.5 * np.sin(2 * np.pi * freq * t) + 0.5 * np.sin(2 * np.pi * freq * 1.5 * t)
    else:
        # ë¬´ë“œì— ë”°ë¥¸ íŒŒí˜• ë³€í™”
        wave_data = (1-mood_v) * np.sin(2 * np.pi * freq * t) + mood_v * np.sign(np.sin(2 * np.pi * freq * t))
    
    return apply_pro_eq(wave_data, layer_idx, mood_v)

uploaded_file = st.file_uploader("ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ìµœì í™” ì™„ë£Œ)", type=['mp4', 'mov', 'avi'])

if uploaded_file:
    # ì„ì‹œ íŒŒì¼ ì €ì¥ (ë©”ëª¨ë¦¬ í™•ë³´)
    temp_path = "temp_video.mp4"
    with open(temp_path, "wb") as f:
        f.write(uploaded_file.read())
    
    # 1. ì˜ìƒ íŠ¹ìƒ‰ ë¶„ì„
    avg_brightness, avg_hue = analyze_video_characteristics(temp_path)
    st.info(f"âœ¨ ì˜ìƒ ë¶„ì„ ì™„ë£Œ: {'ë°ê³  ì°¨ê°€ìš´' if avg_brightness > 0.5 else 'ì–´ë‘¡ê³  ë”°ëœ»í•œ'} ë¬´ë“œì˜ ì‚¬ìš´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    cap = cv2.VideoCapture(temp_path)
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    video_len = total_frames / fps
    
    num_units = int(video_len / UNIT_SEC)
    tracks_l = [np.zeros(int(SAMPLE_RATE * video_len) + 100) for _ in range(4)]
    tracks_r = [np.zeros(int(SAMPLE_RATE * video_len) + 100) for _ in range(4)]
    vis_data = [[] for _ in range(4)]

    prog = st.progress(0)
    for u in range(num_units):
        # ì •í•´ì§„ ë¹„íŠ¸ íƒ€ì´ë°ì˜ í”„ë ˆì„ë§Œ ì •í™•íˆ ì§šì–´ì„œ ë¶„ì„ (ì‚°ë§Œí•¨ ì œê±° í•µì‹¬)
        target_frame = int(u * UNIT_SEC * fps)
        cap.set(cv2.CAP_PROP_POS_FRAMES, target_frame)
        ret, frame = cap.read()
        if not ret: break
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        # ë¹›ì˜ ì„¸ê¸°(Intensity) ê¸°ë°˜ ë™ì  ì„ê³„ê°’
        _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        sorted_cnts = sorted(contours, key=cv2.contourArea, reverse=True)[:4]
        start_s = int(u * UNIT_SEC * SAMPLE_RATE)
        
        for idx, cnt in enumerate(sorted_cnts):
            area = cv2.contourArea(cnt)
            M = cv2.moments(cnt)
            if M["m00"] == 0: continue
            cx = int(M["m10"]/M["m00"])
            
            # ì˜ìƒ ë¬´ë“œì™€ ë ˆì´ì–´ì— ë§ì¶¤í™”ëœ ì£¼íŒŒìˆ˜ (C-Major ê¸°ë°˜)
            base_freqs = [65, 130, 261, 523]
            freq = base_freqs[idx] + (avg_hue * 50) + (area % 20)
            
            tone = generate_wave(freq, UNIT_SEC, idx, avg_brightness)
            
            pan_r = np.clip(cx / frame.shape[1], 0.1, 0.9)
            pan_l = 1.0 - pan_r
            
            end_s = start_s + len(tone)
            if end_s < len(tracks_l[0]):
                tracks_l[idx][start_s:end_s] += tone * pan_l
                tracks_r[idx][start_s:end_s] += tone * pan_r
            vis_data[idx].append(freq)

        for j in range(len(sorted_cnts), 4): vis_data[j].append(None)
        if u % 5 == 0: prog.progress(u / num_units)

    cap.release()

    # --- ë¯¹ì‹± & ë§ˆìŠ¤í„°ë§ ---
    master_l, master_r = np.sum(tracks_l, axis=0), np.sum(tracks_r, axis=0)
    master_stereo = np.vstack((master_l, master_r)).T
    peak = np.max(np.abs(master_stereo))
    if peak > 0: master_stereo = (master_stereo / peak) * 0.8
    
    wav_io = io.BytesIO()
    with wave.open(wav_io, 'wb') as wf:
        wf.setnchannels(2); wf.setsampwidth(2); wf.setframerate(SAMPLE_RATE)
        wf.writeframes((master_stereo * 32767).astype(np.int16).tobytes())

    # --- ê²°ê³¼ UI ---
    col_v, col_g = st.columns([1, 1])
    with col_v:
        st.header("ğŸ Sync View")
        st.video(temp_path)
        st.audio(wav_io.getvalue())
        st.download_button("ğŸ’¾ ì „ì²´ ìŒì› ì €ì¥", wav_io.getvalue(), "optical_pro_mix.wav")

    with col_g:
        st.header("ğŸ“Š MIDI Quantized Graph")
        fig = go.Figure()
        colors = ['#00E5FF', '#FF3D00', '#D500F9', '#FFEA00']
        t_axis = np.linspace(0, video_len, len(vis_data[0]))
        for i in range(4):
            fig.add_trace(go.Scatter(x=t_axis, y=vis_data[i], name=f"Layer {i+1}", line=dict(color=colors[i])))
        fig.update_layout(template="plotly_dark", height=400, xaxis=dict(rangeslider=dict(visible=True)))
        st.plotly_chart(fig, use_container_width=True)

    # ë ˆì´ì–´ë³„ ê°œë³„ ì²­ì·¨ ë° ì €ì¥
    st.divider()
    st.subheader("ğŸ“ Layer Stems")
    cols = st.columns(4)
    for i in range(4):
        with cols[i]:
            l_io = io.BytesIO()
            l_data = np.vstack((tracks_l[i], tracks_r[i])).T
            l_peak = np.max(np.abs(l_data))
            if l_peak > 0: l_data = (l_data / l_peak) * 0.7
            with wave.open(l_io, 'wb') as wf:
                wf.setnchannels(2); wf.setsampwidth(2); wf.setframerate(SAMPLE_RATE)
                wf.writeframes((l_data * 32767).astype(np.int16).tobytes())
            st.write(f"Track {i+1}")
            st.audio(l_io.getvalue())
            st.download_button(f"ğŸ“¥ Layer {i+1} ì €ì¥", l_io.getvalue(), f"layer_{i+1}.wav")
