import streamlit as st
import cv2
import numpy as np
from scipy.io import wavfile
import io
import plotly.graph_objects as go

# --- ì„¤ì • ---
st.set_page_config(layout="wide", page_title="Musical Light DAW")
st.title("ğŸ¼ GarageLight: Optical Synth DAW")

# ìŒê³„ ì„¤ì • (ë§ˆì´ë„ˆ íœíƒ€í† ë‹‰: ë°¤ì˜ ëª½í™˜ì ì¸ ëŠë‚Œ)
NOTES = [130.81, 155.56, 174.61, 196.00, 233.08, 
         261.63, 311.13, 349.23, 392.00, 466.16, 
         523.25, 622.25, 698.46, 783.99, 932.33]

def get_nearest_note(freq):
    return min(NOTES, key=lambda x: abs(x - freq))

def apply_envelope(tone, sample_rate):
    n = len(tone)
    if n < 100: return tone
    attack = int(min(sample_rate * 0.01, n * 0.1))
    release = int(min(sample_rate * 0.05, n * 0.2))
    env = np.ones(n)
    env[:attack] = np.linspace(0, 1, attack)
    env[-release:] = np.linspace(1, 0, release)
    return tone * env

# --- ì‚¬ì´ë“œë°” ì»¨íŠ¸ë¡¤ ---
with st.sidebar:
    st.header("ğŸ› ì»¨íŠ¸ë¡¤ íŒ¨ë„")
    uploaded_file = st.file_uploader("ì˜ìƒì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=['mp4', 'mov', 'avi'])
    st.divider()
    vol_boost = st.slider("ë³¼ë¥¨ ì¦í­ë„ (Gain)", 0.1, 2.0, 1.0, 0.1)
    st.info("ë³¼ë¥¨ì„ ë†’ì—¬ë„ ì—ëŸ¬ê°€ ë‚˜ì§€ ì•Šë„ë¡ ì•ˆì „ ë¦¬ë¯¸í„°ê°€ ì‘ë™í•©ë‹ˆë‹¤.")

if uploaded_file:
    try:
        with open("temp_video.mp4", "wb") as f:
            f.write(uploaded_file.getbuffer())
        
        cap = cv2.VideoCapture("temp_video.mp4")
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps < 1 or np.isnan(fps): fps = 30
        
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample_rate = 22050 
        max_tracks = 6
        
        # ë„‰ë„‰í•œ ê¸¸ì´ì˜ ë§ˆìŠ¤í„° ë°°ì—´ ìƒì„±
        audio_len = int(sample_rate * (total_frames / fps)) + (sample_rate * 2)
        master_l = np.zeros(audio_len, dtype=np.float32)
        master_r = np.zeros(audio_len, dtype=np.float32)
        
        tracks_visual = [[] for _ in range(max_tracks)]
        
        prog_bar = st.progress(0)
        status = st.empty()

        for i in range(total_frames):
            ret, frame = cap.read()
            if not ret: break
            
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            _, thresh = cv2.threshold(gray, 225, 255, cv2.THRESH_BINARY)
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            start_idx = int(i * (sample_rate / fps))
            duration = 1.0 / fps
            t = np.linspace(0, duration, int(sample_rate * duration), False)
            
            sorted_contours = sorted(contours, key=cv2.contourArea, reverse=True)[:max_tracks]
            
            for idx, cnt in enumerate(sorted_contours):
                area = cv2.contourArea(cnt)
                M = cv2.moments(cnt)
                if M["m00"] == 0: continue
                cx, cy = int(M["m10"] / M["m00"]), int(M["m01"] / M["m00"])
                
                # ìŒê³„ ë° ë³¼ë¥¨ ê³„ì‚°
                note_freq = get_nearest_note(150 + ((frame.shape[0] - cy) * 1.5))
                vol = min(area / 1500, 0.5) * vol_boost
                
                # ê¸°ë³¸ìŒ + ë°°ìŒ í•©ì„±
                tone = vol * np.sin(2 * np.pi * note_freq * t)
                tone += (vol * 0.2) * np.sin(2 * np.pi * (note_freq * 2) * t)
                tone = apply_envelope(tone, sample_rate)
                
                # ì…ì²´ ìŒí–¥ (Panning)
                pan_r = cx / frame.shape[1]
                pan_l = 1 - pan_r
                
                end_idx = start_idx + len(tone)
                if end_idx < audio_len:
                    master_l[start_idx:end_idx] += (tone * pan_l)
                    master_r[start_idx:end_idx] += (tone * pan_r)
                
                tracks_visual[idx].append({'time': i/fps, 'freq': note_freq})
            
            if i % 30 == 0:
                prog_bar.progress(min(i / total_frames, 1.0))
                status.text(f"í”„ë ˆì„ ë¶„ì„ ì¤‘: {i}/{total_frames}")

        status.success("âœ… ì‚¬ìš´ë“œ ë Œë”ë§ ì™„ë£Œ!")

        # --- [ì—ëŸ¬ í•´ê²° ë° ë³¼ë¥¨ ìµœì í™” í•µì‹¬ ë¡œì§] ---
        # 1. ìŠ¤í…Œë ˆì˜¤ë¡œ í•©ì¹˜ê¸°
        master_stereo = np.vstack((master_l, master_r)).T
        
        # 2. ë¦¬ë¯¸í„° ë° ë…¸ë©€ë¼ì´ì§• (H format ì—ëŸ¬ ë°©ì§€ í•µì‹¬)
        max_peak = np.max(np.abs(master_stereo))
        
        if max_peak > 0:
            # ì „ì²´ ì†Œë¦¬ë¥¼ -1.0 ~ 1.0 ë²”ìœ„ë¡œ ì••ì¶•
            # ì—¬ê¸°ì— 0.9ë¥¼ ê³±í•´ì£¼ë©´ ìµœëŒ€ ë³¼ë¥¨ì—ì„œë„ ì†Œë¦¬ê°€ ê¹¨ì§€ì§€ ì•ŠëŠ” ì—¬ìœ  ê³µê°„(Headroom)ì´ ìƒê¹ë‹ˆë‹¤.
            master_final = (master_stereo / max_peak) * 0.9
            # ì •ìˆ˜í˜•(int16)ìœ¼ë¡œ í™•ì‹¤í•˜ê²Œ ë³€í™˜
            audio_out = (master_final * 32767).astype(np.int16)
        else:
            audio_out = master_stereo.astype(np.int16)

        # --- UI ì¶œë ¥ ---
        col1, col2 = st.columns([1, 1])
        with col1:
            st.header("ğŸ View & Play")
            st.video(uploaded_file)
            st.audio(audio_out, sample_rate=sample_rate)
            
            buf = io.BytesIO()
            wavfile.write(buf, sample_rate, audio_out)
            st.download_button("ğŸ’¾ Download Master (WAV)", buf.getvalue(), "musical_bus.wav")

        with col2:
            st.header("ğŸ“Š Harmonic Timeline")
            for idx in range(max_tracks):
                if tracks_visual[idx]:
                    v_times = [v['time'] for v in tracks_visual[idx]]
                    v_freqs = [v['freq'] for v in tracks_visual[idx]]
                    fig = go.Figure(go.Scatter(x=v_times, y=v_freqs, mode='lines', line=dict(color='#00d1ff', width=1)))
                    fig.update_layout(height=100, margin=dict(l=0,r=0,t=10,b=10), xaxis_visible=False, paper_bgcolor="rgba(0,0,0,0)", plot_bgcolor="rgba(0,0,0,0)")
                    st.plotly_chart(fig, use_container_width=True)

    except Exception as e:
        st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
